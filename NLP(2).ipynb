{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#***Stemming***\n",
        "*Stemming is the process of reducing a word to its word stem*\n",
        "\n",
        "**Example**\n",
        "\n",
        "\n",
        "1.  \"running\" -> \"run\"\n",
        "2.   \"happiness\" -> \"happi\"\n",
        "3. \"caresses\" -> \"caress\"\n",
        "\n",
        "#***Overstemming***\n",
        "\n",
        "*Overstemming occurs when a stemming algorithm removes more characters than necessary, leading to stems that are too general or incorrect*.\n",
        "\n",
        "**Example**\n",
        "\n",
        "1.  \"university\" -> \"univers\" (correct stem: \"universi\")\n",
        "\n",
        "2.   \"generalization\" -> \"gener\" (correct stem: \"general\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SX3DvTsGqUKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7N6PJj4rvmX",
        "outputId": "62dcc322-0dbb-4e46-83ef-4f769583a0f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemming = PorterStemmer()\n",
        "\n",
        "words=[\"eating!\",'eating',\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n",
        "\n",
        "for word in words:\n",
        "    print(word+\"   ----> \"+'  '+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "De0dArCyrqC6",
        "outputId": "75b57707-2f2f-4ade-feaf-5f5dbae896b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating!   ---->   eating!\n",
            "eating   ---->   eat\n",
            "eats   ---->   eat\n",
            "eaten   ---->   eaten\n",
            "writing   ---->   write\n",
            "writes   ---->   write\n",
            "programming   ---->   program\n",
            "programs   ---->   program\n",
            "history   ---->   histori\n",
            "finally   ---->   final\n",
            "finalized   ---->   final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('Congratulation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "KGSCTt2RrqGq",
        "outputId": "6719b3b1-79cb-43fe-e52b-517a1501dc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"sitting\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bepNnn8wrqJy",
        "outputId": "9b32338d-bd01-4b91-a0a2-9228e966dd38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sit'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFcT9hPApzeu",
        "outputId": "7a7b69e0-bee4-4b65-c53c-c6e7c1a2e74e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running  ---->  run\n",
            "happiness  ---->  happy\n",
            "caresses  ---->  caress\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "words = [\"running\", \"happiness\", \"caresses\"]\n",
        "for word in words:\n",
        "    print(word+\"  ----> \"+' '+lancaster.stem(word))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lancaster.stem('Congratulation')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "lH8jWEkpstyH",
        "outputId": "e49dc419-3a9f-45d7-fc5f-44a4c1ae349a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congrat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***RegexpStemmer class***\n",
        "*NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms*"
      ],
      "metadata": {
        "id": "NKEohB7Ls7IH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)\n",
        "\n",
        "print(reg_stemmer.stem('eating'))\n",
        "print(reg_stemmer.stem('ingeating'))\n",
        "print(reg_stemmer.stem('eats'))\n",
        "print(reg_stemmer.stem('vegetable'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HAZO1kIsztu",
        "outputId": "cfc3f6a5-6523-486c-e9e8-5bb4dcfaf008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eat\n",
            "ingeat\n",
            "eat\n",
            "veget\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SnowballStemmer***. *The following languages are supported: Arabic, Danish, Dutch, English, Finnish, French, German, Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian, Spanish and Swedish.*"
      ],
      "metadata": {
        "id": "mxdS-TxotuiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowballsstemmer=SnowballStemmer('english')\n",
        "\n",
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n",
        "for word in words:\n",
        "    print(word+\"  ----> \"+' '+snowballsstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfFADv4FtAzV",
        "outputId": "945ac651-214e-4823-82c2-ed75c47aefc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating  ---->  eat\n",
            "eats  ---->  eat\n",
            "eaten  ---->  eaten\n",
            "writing  ---->  write\n",
            "writes  ---->  write\n",
            "programming  ---->  program\n",
            "programs  ---->  program\n",
            "history  ---->  histori\n",
            "finally  ---->  final\n",
            "finalized  ---->  final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "porter_stems = [\"fairly\", \"sportingly\", \"goes\"]\n",
        "snowball_stems = [\"fairly\", \"sportingly\", \"goes\"]\n",
        "\n",
        "Porter = []\n",
        "Snowball = []\n",
        "for i in porter_stems:\n",
        "  Porter.append(stemming.stem(i))\n",
        "\n",
        "for i in snowball_stems:\n",
        "  Snowball.append(snowballsstemmer.stem(i))\n",
        "\n",
        "print(\"Porter Stemmer: \", Porter)\n",
        "print(\"Snowball Stemmer: \", Snowball)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yOJ5e7JMtcli",
        "outputId": "5b41fbdb-453f-415a-b3c0-db2743ffa03f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer:  ['fairli', 'sportingli', 'goe']\n",
            "Snowball Stemmer:  ['fair', 'sport', 'goe']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Lemmatization***\n",
        "*Lemmatization is a process in natural language processing (NLP) that reduces words to their base or dictionary form, known as the lemma*.\n",
        "\n",
        "*which often removes suffixes in a crude way, lemmatization uses linguistic knowledge, including vocabulary and morphological analysis, to produce more accurate and meaningful base forms*\n",
        "\n",
        "**Example**\n",
        "\n",
        "Running:\n",
        "Lemmatized: \"run\" (verb)\n",
        "\n",
        "Better:\n",
        "Lemmatized: \"good\" (adjective)\n",
        "\n",
        "Geese:\n",
        "Lemmatized: \"goose\" (noun)\n"
      ],
      "metadata": {
        "id": "LL55xnMivNw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2zVZx2xwJkJ",
        "outputId": "920e03f4-4ad9-45b1-c2e8-db4af3a3dca0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "lemmatizer.lemmatize(\"going\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Byaw2qK1t_n_",
        "outputId": "31cb89ff-e97c-4159-c4f0-b9ff4093b828"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'going'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS tag -\n",
        "\n",
        "Noun-n\n",
        "\n",
        "verb-v\n",
        "\n",
        "adjective-a\n",
        "\n",
        "adverb-r"
      ],
      "metadata": {
        "id": "29PjBqDfwOxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(lemmatizer.lemmatize(\"going\",pos='v'))\n",
        "print(lemmatizer.lemmatize(\"going\",pos='n'))\n",
        "print(lemmatizer.lemmatize(\"going\",pos='a'))\n",
        "print(lemmatizer.lemmatize(\"going\",pos='r'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfe3WI-fwGS4",
        "outputId": "affb2213-b5d2-4c51-d35e-7f267590758e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "go\n",
            "going\n",
            "going\n",
            "going\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]\n",
        "\n",
        "for word in words:\n",
        "    print(word+\" ---> \"+' '+lemmatizer.lemmatize(word,pos='n'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZQCMk7LwYJ6",
        "outputId": "5988a6d5-63da-4f7a-9b38-92b05cc17677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating --->  eating\n",
            "eats --->  eats\n",
            "eaten --->  eaten\n",
            "writing --->  writing\n",
            "writes --->  writes\n",
            "programming --->  programming\n",
            "programs --->  program\n",
            "history --->  history\n",
            "finally --->  finally\n",
            "finalized --->  finalized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Lemmatization v/s Stemming***\n",
        "\n",
        "**Stemming**: \"running\" -> \"run\", \"better\" -> \"better\", \"geese\" -> \"gees\"\n",
        "\n",
        "**Lemmatization**: \"running\" -> \"run\", \"better\" -> \"good\", \"geese\" -> \"goose\""
      ],
      "metadata": {
        "id": "18pOfFILw4Fs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#***Stopwords***\n",
        "\n",
        "*Stopwords are common words that usually do not carry significant meaning and are often removed during text preprocessing in natural language processing (NLP) tasks*.\n",
        "\n",
        "**Examples** of stopwords\n",
        "\n",
        "\"and,\" \"is,\" \"in,\" \"the,\"\n",
        "\n",
        "Articles: a, an, the\n",
        "\n",
        "Conjunctions: and, but, or\n",
        "\n",
        "Prepositions: in, on, at\n",
        "\n",
        "Pronouns: he, she, it, they"
      ],
      "metadata": {
        "id": "zI1sDK86xUUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LisbD8daxmin",
        "outputId": "d4985128-8f0a-4774-f840-7dc13697abd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')\n",
        "sentence = 'hi this is a apple'\n",
        "print"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GcjGueyxokT",
        "outputId": "c625d084-499a-4491-c68b-6ee0bc581558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**if word.lower() not in removing_words**\n",
        "\n",
        "*checking all the words in the sentence to in lowercase bcoz . all the stopwords are in lowercase*\n",
        "\n",
        "\n",
        "*ensuring not removing the words that are not presented in stopwords*\n",
        "\n",
        "\n",
        "\n",
        "Checking each word:\n",
        "\n",
        "\"hi\" -> \"hi\" (not in stopwords)\n",
        "\n",
        "\"this\" -> \"this\" (in stopwords)\n",
        "\n",
        "\"is\" -> \"is\" (in stopwords)\n",
        "\n",
        "\"a\" -> \"a\" (in stopwords)\n",
        "\n",
        "\"apple\" -> \"apple\" (not in stopwords)"
      ],
      "metadata": {
        "id": "BGYQuDakzVPn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "removing_words = stopwords.words('english')\n",
        "sentence = 'hi this is a apple'\n",
        "\n",
        "words = sentence.split()\n",
        "print(words)\n",
        "\n",
        "filtered_words = []\n",
        "for word in words:\n",
        "\n",
        "    if word.lower() not in removing_words:\n",
        "        filtered_words.append(word)\n",
        "\n",
        "filtered_sentence = (filtered_words)\n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VHGBjHcxqil",
        "outputId": "69c69b45-49ed-4dd8-d098-c804598dc8af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', 'this', 'is', 'a', 'apple']\n",
            "['hi', 'apple']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***using Join***\n"
      ],
      "metadata": {
        "id": "DRYXZlux0J3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = sentence.split()\n",
        "print(words)\n",
        "\n",
        "filtered_words = []\n",
        "for word in words:\n",
        "\n",
        "    if word.lower() not in removing_words:\n",
        "        filtered_words.append(word)\n",
        "\n",
        "filtered_sentence = ' '.join(filtered_words)\n",
        "\n",
        "print(filtered_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gzu7LpSEyG62",
        "outputId": "abdd9a58-cacd-48ee-a099-e3f192802433"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', 'this', 'is', 'a', 'apple']\n",
            "hi apple\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aUzW13_M0QET"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
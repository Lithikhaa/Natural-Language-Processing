{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#***Natural Language Processing***\n",
        "*NLP is like teaching computers to understand and talk in human language. It helps them read, write, and make sense of text and speech, so they can interact with us more naturally.*\n",
        "\n",
        "\n",
        "***Corpus:***\n",
        "*A large collection of documents used for training NLP models.*"
      ],
      "metadata": {
        "id": "K4OLOfm6dijQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpocpK8hPTZ6",
        "outputId": "61df3e6c-1af4-4027-aeb6-2a5a86404997"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The corpus:  ['Iam Lithikhaa', 'Iam a student at the moment']\n",
            "The Document:  Iam Lithikhaa\n",
            "The Words:  ['Interested', 'in', 'machinelearning', 'field']\n"
          ]
        }
      ],
      "source": [
        "# Example\n",
        "\n",
        "corpus = ['Iam Lithikhaa','Iam a student at the moment']\n",
        "print('The corpus: ',corpus)\n",
        "\n",
        "document = 'Iam Lithikhaa'\n",
        "print('The Document: ',document)\n",
        "\n",
        "words = ['Interested','in','machinelearning','field']\n",
        "print('The Words: ',words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Tokens***\n",
        "\n",
        "*When processing text, the text is split into these smaller pieces(tokens) to analyze and understand the content better*"
      ],
      "metadata": {
        "id": "CRyhFdTTga1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYEdgYjthb9Q",
        "outputId": "3a843868-d27a-4265-e391-d8f070ce851e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***punkt***\n",
        "\n",
        " *a pre-trained model used for sentence boundary detection also known as sentence tokenization*"
      ],
      "metadata": {
        "id": "jaGJOBp2hvXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "sentence = \"Iam a student at the moment!!\"\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens: \", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0TT-dQtfo2q",
        "outputId": "13aed4dc-4eb7-43f9-84c1-2fc976ea6834"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['Iam', 'a', 'student', 'at', 'the', 'moment', '!', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Vocabulary***\n",
        "\n",
        "*The set of unique tokens found in a corpus. This represents the words and symbols that the model or algorithm has been trained to recognize and understand.*"
      ],
      "metadata": {
        "id": "dOi91yE2icGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Iam a student at the moment in a college!!\"\n",
        "\n",
        "tokens = word_tokenize(sentence)\n",
        "print(\"Tokens: \", tokens)\n",
        "\n",
        "#used to get the unique values using set\n",
        "Vocabulary = set(tokens)\n",
        "print(\"Vocabulary: \", Vocabulary)\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdcZSKZ2hQ8h",
        "outputId": "54b99002-6a64-4993-b614-7c8ebbcfa06c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:  ['Iam', 'a', 'student', 'at', 'the', 'moment', 'in', 'a', 'college', '!', '!']\n",
            "Vocabulary:  {'Iam', 'a', 'in', 'moment', 'college', '!', 'student', 'at', 'the'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "line = \"\"\" Sometimes we get bored:( ,so\n",
        "just do whatever makes you happy.\"\"\"\n",
        "\n",
        "wordpunct_tokenize(line)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cgHGDUcjKQk",
        "outputId": "d6dd75fb-ed12-4a01-cdb3-e8be44fc249f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sometimes',\n",
              " 'we',\n",
              " 'get',\n",
              " 'bored',\n",
              " ':(',\n",
              " ',',\n",
              " 'so',\n",
              " 'just',\n",
              " 'do',\n",
              " 'whatever',\n",
              " 'makes',\n",
              " 'you',\n",
              " 'happy',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Difference Between Wordpunct and word_tokenize*"
      ],
      "metadata": {
        "id": "DVsd4KMKktF-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\" Sometimes    we get bored:( ,so\n",
        "just do whatever makes you happy.\"\"\"\n",
        "\n",
        "# Using wordpunct_tokenize\n",
        "tokens_wp = wordpunct_tokenize(text)\n",
        "print(\"wordpunct_tokenize:\", tokens_wp)\n",
        "\n",
        "\n",
        "# Using word_tokenize\n",
        "tokens_wt = word_tokenize(text)\n",
        "print(\"word_tokenize:\", tokens_wt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYRztLdGkWem",
        "outputId": "184d490f-ba47-46bf-c446-eea470df7606"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wordpunct_tokenize: ['Sometimes', 'we', 'get', 'bored', ':(', ',', 'so', 'just', 'do', 'whatever', 'makes', 'you', 'happy', '.']\n",
            "word_tokenize: ['Sometimes', 'we', 'get', 'bored', ':', '(', ',', 'so', 'just', 'do', 'whatever', 'makes', 'you', 'happy', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Sentence Tokenization***\n",
        "\n",
        "*Splits text into individual sentences based on punctuation marks like periods, exclamation points, or question marks*"
      ],
      "metadata": {
        "id": "uhdpOw2xlmiE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\" Sometimes    we get bored:( ,so\n",
        "just do whatever makes you happy.\"\"\"\n",
        "\n",
        "sentence = nltk.sent_tokenize(text)\n",
        "print(\"wordpunct_tokenize:\", sentence)\n",
        "\n",
        "text = \" Sometimes we get bored!. so just do whatever makes you happy.\"\n",
        "\n",
        "sentence = nltk.sent_tokenize(text)\n",
        "print(\"wordpunct_tokenize:\", sentence)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9LcAvVFlNn7",
        "outputId": "f48e39f1-5d3e-4b07-c8a8-62b8acbeb58c"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wordpunct_tokenize: [' Sometimes    we get bored:( ,so\\njust do whatever makes you happy.']\n",
            "wordpunct_tokenize: [' Sometimes we get bored!.', 'so just do whatever makes you happy.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import WhitespaceTokenizer\n",
        "\n",
        "text = \" Sometimes we get bored  !. so just do whatever makes     you happy.\"\n",
        "whitespace_tokenizer = WhitespaceTokenizer()\n",
        "whitespace_tokens = whitespace_tokenizer.tokenize(text)\n",
        "\n",
        "print(\"Whitespace Tokenization: \", whitespace_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVLXW-Rcl-US",
        "outputId": "657c836e-cc40-4083-98fc-92ef305347c8"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Whitespace Tokenization:  ['Sometimes', 'we', 'get', 'bored', '!.', 'so', 'just', 'do', 'whatever', 'makes', 'you', 'happy.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***NGram Tokenization***\n",
        "\n",
        "*Creates tokens by combining N consecutive words from the text*."
      ],
      "metadata": {
        "id": "j59hRMyOnI-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "text = \"Sometimes we get bored!. so just do whatever makes you happy. \"\n",
        "\n",
        "word_tokens = word_tokenize(text)\n",
        "n = 3\n",
        "ngram_tokens = list(ngrams(word_tokens, n))\n",
        "print(\"N-Gram Tokenization : \", ngram_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBr1SHndmquC",
        "outputId": "f86bb158-def2-4f71-cc97-dda11b3e7b8d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N-Gram Tokenization :  [('Sometimes', 'we', 'get'), ('we', 'get', 'bored'), ('get', 'bored', '!'), ('bored', '!', '.'), ('!', '.', 'so'), ('.', 'so', 'just'), ('so', 'just', 'do'), ('just', 'do', 'whatever'), ('do', 'whatever', 'makes'), ('whatever', 'makes', 'you'), ('makes', 'you', 'happy'), ('you', 'happy', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Custom Tokenization***\n",
        "\n",
        "*Tailors tokenization rules based on specific requirements or domain-specific knowledge*.\n",
        "\n",
        "\n",
        "**re.IGNORECASE**: This flag makes the search case-insensitive, meaning it will match \"Heart,\" \"HEART,\" and \"heart\" equally\n",
        "\n",
        "**findall**: To find the match patterns\n",
        "\n",
        "**literal characters**: are those that are written exactly as they are,\n",
        "In this string, H, e, l, l, o, ,, , W, o, r, l, d, and ! are all literal characters.\n",
        "\n",
        "**r**:\n",
        "r stands for \"raw string.\"\n",
        "In regular expressions, the backslash (\\) is a special character used to escape other characters (e.g., \\n for a newline or \\d for a digit). When writing regular expressions, you often need to use many backslashes, which can make the pattern hard to read and write.\n",
        "\n",
        "By using a raw string (indicated by r'...'), Python treats backslashes as literal characters and does not apply special meaning to them.\n",
        "\n",
        "**For example**:\n",
        "\n",
        "without_r = \"This is a newline character: \\\\n\"\n",
        "\n",
        "with_r = r\"This is a newline character: \\n\""
      ],
      "metadata": {
        "id": "M-ROzylQn3Ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''Example: Suppose you want to match a string that includes either\n",
        " \"cat\" or \"dog\" followed by \"house\", but you donâ€™t need to capture \"cat\" or \"dog\" separately.'''\n",
        "\n",
        "import re\n",
        "pattern = r'(?:cat|dog) house'\n",
        "text = 'I have a cat house and a dog house.'\n",
        "\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)\n",
        "\n",
        "\n",
        "# Example: Suppose you want to match the word \"cat\" but not \"catalog\".\n",
        "pattern = r'\\bcat\\b'\n",
        "text = 'The cat is on the catalog.'\n",
        "\n",
        "matches = re.findall(pattern, text)\n",
        "print(matches)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ur5uTiztU49",
        "outputId": "d6212a8c-0472-4b3b-f528-9240f2de3a85"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cat house', 'dog house']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def custom_tokenize(text):\n",
        "    medical_terms_pattern = r'(?:\\b(?:heart|lung|brain)\\b)|(?:\\b(?:COPD|MRI|ECG)\\b)'\n",
        "    tokens = re.findall(medical_terms_pattern, text, flags=re.IGNORECASE)\n",
        "    return tokens\n",
        "medical_text = \"The patient underwent an MRI scan to examine the brain. COPD is a chronic lung disease.\"\n",
        "custom_tokens = custom_tokenize(medical_text)\n",
        "print(\"Custom Tokenization (Medical Terms and Abbreviations):  \", custom_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gccAvzjnQWn",
        "outputId": "916b835f-f06c-4671-97e0-ee6bb6742c80"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Custom Tokenization (Medical Terms and Abbreviations):   ['MRI', 'brain', 'COPD', 'lung']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aSSjOpJ0oAiG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}